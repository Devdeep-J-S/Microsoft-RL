Name : Devdeep Shetranjiwala <br>
Email ID : devdeep0702@gmail.com 

****

# Microsoft Reinforcement Learning Open-Source Fest - 2023
# Feature generation using transformers - Screening exercise

---

## Screening exercise
> When submitting your application please also complete the following exercise.
Write a Jupyter Notebook to conducting a small task with a transformer and explain what you are trying to solve.

(Please check the installation, examples, and tutorial if needed: https://huggingface.co/docs/transformers/index)

## Explanation

> * The goal of this task was to classify sentences as either grammatically correct or incorrect using a pre-trained transformer model from the Hugging Face Transformers library, fine-tuned on the CoLA dataset. The CoLA dataset is a corpus of English sentences labeled with a binary acceptability judgment indicating whether the sentence is grammatically correct or incorrect. The task involves natural language processing (NLP) and binary classification.

> * We used the BERT (Bidirectional Encoder Representations from Transformers) model, which is a pre-trained transformer model that has achieved state-of-the-art results on a wide range of NLP tasks. We fine-tuned the pre-trained BERT model on the CoLA dataset using TensorFlow, which is an open-source platform for machine learning that provides a high-level API for building and training machine learning models.

> * By fine-tuning the pre-trained BERT model on the CoLA dataset, we were able to leverage the pre-trained model's knowledge of natural language to improve the accuracy of our classification task. We evaluated the performance of the model on the validation dataset and used it to make predictions on new sentences.

> * The ability to accurately classify sentences as grammatically correct or incorrect has many practical applications in NLP, such as in automated essay grading, grammar checking, and language translation.
